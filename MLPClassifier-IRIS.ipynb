{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+Gy126JP4CUpXdtQa713+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBWBbSXOR78P","executionInfo":{"status":"ok","timestamp":1734770035259,"user_tz":-540,"elapsed":1752,"user":{"displayName":"장문경","userId":"10668280226191651710"}},"outputId":"36688d61-2346-4190-c290-1be395ccac6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 1.57720223\n","Iteration 2, loss = 1.18912655\n","Iteration 3, loss = 1.17785660\n","Iteration 4, loss = 1.14885589\n","Iteration 5, loss = 1.12780719\n","Iteration 6, loss = 1.09287610\n","Iteration 7, loss = 1.06795289\n","Iteration 8, loss = 1.03592415\n","Iteration 9, loss = 0.99971938\n","Iteration 10, loss = 0.96284139\n","Iteration 11, loss = 0.91801008\n","Iteration 12, loss = 0.87434419\n","Iteration 13, loss = 0.83318694\n","Iteration 14, loss = 0.79748651\n","Iteration 15, loss = 0.76586457\n","Iteration 16, loss = 0.73851470\n","Iteration 17, loss = 0.71603499\n","Iteration 18, loss = 0.69634375\n","Iteration 19, loss = 0.68036640\n","Iteration 20, loss = 0.66472952\n","Iteration 21, loss = 0.65168936\n","Iteration 22, loss = 0.63895148\n","Iteration 23, loss = 0.62798991\n","Iteration 24, loss = 0.61899230\n","Iteration 25, loss = 0.60962600\n","Iteration 26, loss = 0.60226087\n","Iteration 27, loss = 0.59475331\n","Iteration 28, loss = 0.58745510\n","Iteration 29, loss = 0.58145207\n","Iteration 30, loss = 0.57561943\n","Iteration 31, loss = 0.57031569\n","Iteration 32, loss = 0.56567116\n","Iteration 33, loss = 0.56098849\n","Iteration 34, loss = 0.55692120\n","Iteration 35, loss = 0.55305455\n","Iteration 36, loss = 0.54924839\n","Iteration 37, loss = 0.54572770\n","Iteration 38, loss = 0.54230645\n","Iteration 39, loss = 0.53995097\n","Iteration 40, loss = 0.53694735\n","Iteration 41, loss = 0.53405045\n","Iteration 42, loss = 0.53171880\n","Iteration 43, loss = 0.52955911\n","Iteration 44, loss = 0.52758721\n","Iteration 45, loss = 0.52559844\n","Iteration 46, loss = 0.52362335\n","Iteration 47, loss = 0.52154101\n","Iteration 48, loss = 0.51988652\n","Iteration 49, loss = 0.51814118\n","Iteration 50, loss = 0.51685250\n","Iteration 51, loss = 0.51531684\n","Iteration 52, loss = 0.51365364\n","Iteration 53, loss = 0.51241554\n","Iteration 54, loss = 0.51103820\n","Iteration 55, loss = 0.50977111\n","Iteration 56, loss = 0.50858508\n","Iteration 57, loss = 0.50751975\n","Iteration 58, loss = 0.50646417\n","Iteration 59, loss = 0.50542405\n","Iteration 60, loss = 0.50436302\n","Iteration 61, loss = 0.50348815\n","Iteration 62, loss = 0.50268917\n","Iteration 63, loss = 0.50161776\n","Iteration 64, loss = 0.50085879\n","Iteration 65, loss = 0.49993413\n","Iteration 66, loss = 0.49918504\n","Iteration 67, loss = 0.49873403\n","Iteration 68, loss = 0.49779500\n","Iteration 69, loss = 0.49777206\n","Iteration 70, loss = 0.49673587\n","Iteration 71, loss = 0.49617170\n","Iteration 72, loss = 0.49551458\n","Iteration 73, loss = 0.49512798\n","Iteration 74, loss = 0.49463800\n","Iteration 75, loss = 0.49387414\n","Iteration 76, loss = 0.49312738\n","Iteration 77, loss = 0.49249978\n","Iteration 78, loss = 0.49252657\n","Iteration 79, loss = 0.49162070\n","Iteration 80, loss = 0.49123905\n","Iteration 81, loss = 0.49061032\n","Iteration 82, loss = 0.49004186\n","Iteration 83, loss = 0.48952266\n","Iteration 84, loss = 0.48942041\n","Iteration 85, loss = 0.48865766\n","Iteration 86, loss = 0.48820353\n","Iteration 87, loss = 0.48787236\n","Iteration 88, loss = 0.48759609\n","Iteration 89, loss = 0.48713780\n","Iteration 90, loss = 0.48677410\n","Iteration 91, loss = 0.48648532\n","Iteration 92, loss = 0.48620793\n","Iteration 93, loss = 0.48571174\n","Iteration 94, loss = 0.48553503\n","Iteration 95, loss = 0.48501647\n","Iteration 96, loss = 0.48479721\n","Iteration 97, loss = 0.48478198\n","Iteration 98, loss = 0.48444905\n","Iteration 99, loss = 0.48463044\n","Iteration 100, loss = 0.48401212\n","Iteration 101, loss = 0.48367251\n","Iteration 102, loss = 0.48330366\n","Iteration 103, loss = 0.48292985\n","Iteration 104, loss = 0.48264393\n","Iteration 105, loss = 0.48239273\n","Iteration 106, loss = 0.48220979\n","Iteration 107, loss = 0.48193939\n","Iteration 108, loss = 0.48167829\n","Iteration 109, loss = 0.48146053\n","Iteration 110, loss = 0.48128793\n","Iteration 111, loss = 0.48109599\n","Iteration 112, loss = 0.48082791\n","Iteration 113, loss = 0.48062406\n","Iteration 114, loss = 0.48045495\n","Iteration 115, loss = 0.48024935\n","Iteration 116, loss = 0.48019260\n","Iteration 117, loss = 0.48003993\n","Iteration 118, loss = 0.47992151\n","Iteration 119, loss = 0.47964278\n","Iteration 120, loss = 0.47946586\n","Iteration 121, loss = 0.47931024\n","Iteration 122, loss = 0.47926353\n","Iteration 123, loss = 0.47892871\n","Iteration 124, loss = 0.47884038\n","Iteration 125, loss = 0.47858586\n","Iteration 126, loss = 0.47839217\n","Iteration 127, loss = 0.47822532\n","Iteration 128, loss = 0.47812436\n","Iteration 129, loss = 0.47802215\n","Iteration 130, loss = 0.47773156\n","Iteration 131, loss = 0.47775059\n","Iteration 132, loss = 0.47746364\n","Iteration 133, loss = 0.47761744\n","Iteration 134, loss = 0.47765701\n","Iteration 135, loss = 0.47764158\n","Iteration 136, loss = 0.47748262\n","Iteration 137, loss = 0.47723184\n","Iteration 138, loss = 0.47706731\n","Iteration 139, loss = 0.47670299\n","Iteration 140, loss = 0.47658753\n","Iteration 141, loss = 0.47635273\n","Iteration 142, loss = 0.47645007\n","Iteration 143, loss = 0.47642860\n","Iteration 144, loss = 0.47606245\n","Iteration 145, loss = 0.47591866\n","Iteration 146, loss = 0.47578355\n","Iteration 147, loss = 0.47566072\n","Iteration 148, loss = 0.47601643\n","Iteration 149, loss = 0.47548219\n","Iteration 150, loss = 0.47564672\n","Iteration 151, loss = 0.47551745\n","Iteration 152, loss = 0.47565661\n","Iteration 153, loss = 0.47560883\n","Iteration 154, loss = 0.47538330\n","Iteration 155, loss = 0.47539978\n","Iteration 156, loss = 0.47535273\n","Iteration 157, loss = 0.47532510\n","Iteration 158, loss = 0.47514321\n","Iteration 159, loss = 0.47527347\n","Iteration 160, loss = 0.47483893\n","Iteration 161, loss = 0.47488205\n","Iteration 162, loss = 0.47477379\n","Iteration 163, loss = 0.47481687\n","Iteration 164, loss = 0.47428729\n","Iteration 165, loss = 0.47423076\n","Iteration 166, loss = 0.47412537\n","Iteration 167, loss = 0.47402713\n","Iteration 168, loss = 0.47393566\n","Iteration 169, loss = 0.47390432\n","Iteration 170, loss = 0.47382500\n","Iteration 171, loss = 0.47361672\n","Iteration 172, loss = 0.47360899\n","Iteration 173, loss = 0.47361943\n","Iteration 174, loss = 0.47340631\n","Iteration 175, loss = 0.47336606\n","Iteration 176, loss = 0.47333147\n","Iteration 177, loss = 0.47351190\n","Iteration 178, loss = 0.47342378\n","Iteration 179, loss = 0.47340212\n","Iteration 180, loss = 0.47364808\n","Iteration 181, loss = 0.47312092\n","Iteration 182, loss = 0.47316262\n","Iteration 183, loss = 0.47292360\n","Iteration 184, loss = 0.47322202\n","Iteration 185, loss = 0.47272511\n","Iteration 186, loss = 0.47307768\n","Iteration 187, loss = 0.47257498\n","Iteration 188, loss = 0.47262254\n","Iteration 189, loss = 0.47254296\n","Iteration 190, loss = 0.47231197\n","Iteration 191, loss = 0.47236851\n","Iteration 192, loss = 0.47238678\n","Iteration 193, loss = 0.47223819\n","Iteration 194, loss = 0.47220516\n","Iteration 195, loss = 0.47203922\n","Iteration 196, loss = 0.47211803\n","Iteration 197, loss = 0.47234766\n","Iteration 198, loss = 0.47225969\n","Iteration 199, loss = 0.47220357\n","Iteration 200, loss = 0.47198967\n","Iteration 201, loss = 0.47230691\n","Iteration 202, loss = 0.47187668\n","Iteration 203, loss = 0.47181656\n","Iteration 204, loss = 0.47170924\n","Iteration 205, loss = 0.47173219\n","Iteration 206, loss = 0.47170082\n","Iteration 207, loss = 0.47194063\n","Iteration 208, loss = 0.47122354\n","Iteration 209, loss = 0.47151376\n","Iteration 210, loss = 0.47221817\n","Iteration 211, loss = 0.47174522\n","Iteration 212, loss = 0.47180353\n","Iteration 213, loss = 0.47193847\n","Iteration 214, loss = 0.47161817\n","Iteration 215, loss = 0.47152834\n","Iteration 216, loss = 0.47141067\n","Iteration 217, loss = 0.47135446\n","Iteration 218, loss = 0.47159108\n","Iteration 219, loss = 0.47160092\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","[[16.  0.  0.]\n"," [ 0.  0.  0.]\n"," [ 0. 18. 11.]]\n","테스트 집합에 대한 정확률은 60.00%입니다.\n"]}],"source":["from sklearn import datasets\n","from sklearn.datasets import load_iris\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","X, y = load_iris(return_X_y = True)\n","x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, train_size = 0.7)\n","\n","#MLP분류기 모델을 학습\n","mlp = MLPClassifier(hidden_layer_sizes = (5,5,3),\n","                    activation = 'relu',\n","                    learning_rate_init = 0.01,\n","                    batch_size = 24,\n","                    max_iter = 300,\n","                    solver = 'adam', verbose = True)\n","mlp.fit(x_train, y_train)\n","\n","res = mlp.predict(x_test) #테스트 집합으로 예측\n","\n","# 혼동 행렬\n","conf = np.zeros((3,3))\n","for i in range(len(res)):\n","  conf[res[i]][y_test[i]] += 1\n","print(conf)\n","\n","#정확률 계산\n","no_correct = 0\n","for i in range(3):\n","  no_correct+=conf[i][i]\n","accuracy = no_correct/len(res)\n","print(\"테스트 집합에 대한 정확률은 {0:.2f}%입니다.\".format(accuracy * 100))"]}]}